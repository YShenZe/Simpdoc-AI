---
title: 在旧Andriod设备上本地化部署Qwen-1.5-0.5B开源模型
tags:
  - Qwen
  - 本地部署AI
  - DeepSeek
categories: 日常折腾
date: 2025-03-20 12:28:31
---

最近各种大模型的爆火，网上出现了大量的DeepSeek-R1大模型部署教程，但网络上大部分教程都是针对Windows/LinuxPC等高性能PC端的本地化部署教程，很少有针对移动端如旧Andriod设备部署0.5B低参数大模型，我本人对Termux有一定使用经验，所以就一直想用Termux来部署这个`Qwen-1.5-0.5B`开源大模型玩玩，但是一直没时间，最近有空了就来耍，顺带给各位出个教程 ~~水一篇文章~~ ，那么本文就用我那台ViVO Y3+Termux来部署`Qwen-1.5-0.5B`开源大模型同时给出教程和Q＆A。

Qwen1.5-0.5B-Chat-GGUF是一款基于Transformer架构的高效语言模型，支持多种模型尺寸，具备强大的人机对话能力，支持32K的上下文长度，多语言兼容。此模型在人类偏好上有显著提升，是自然语言处理领域的杰出成果。

## 材料准备
在开始之前，你要确保你有
- 一台安装了Termux的旧Andriod设备
- 脑子和手

同时你还需要有Termux基础使用经验，纯小白慎看。

## 初始化Termux环境（可选）
先把Termux的PKG源换成国内的镜像源，这样可以加快下载速度同时可以避免因为网络问题下载失败。
**清华源**
```sh
sed -i 's@^\(deb.*stable main\)$@#\1\ndeb https://mirrors.tuna.tsinghua.edu.cn/termux/termux-packages-24 stable main@' $PREFIX/etc/apt/sources.list && apt update && apt upgrade
```

**北京源**
```sh
sed -i 's@^\(deb.*stable main\)$@#\1\ndeb https://mirrors.bfsu.edu.cn/termux/termux-packages-24 stable main@' $PREFIX/etc/apt/sources.list &&apt update && apt upgrade
```

这里随便选一个就像，我这边就选清华源了。

切换源过程中会遇到这个
![选项1](https://cdn.mengze.vip/gh/YShenZe/Blog-Static-Resource@main/images/IMG20250320090021.jpg)
输入Y并回车继续这其实就是再问你是否需要确定安装。

接下来遇到这五个选项全部直接回车不用选择，默认为N
![IMG20250320090512.jpg](https://cdn.mengze.vip/gh/YShenZe/Blog-Static-Resource@main/images/IMG20250320090512.jpg)
![IMG20250320090503.jpg](https://cdn.mengze.vip/gh/YShenZe/Blog-Static-Resource@main/images/IMG20250320090503.jpg)
![IMG20250320090451.jpg](https://cdn.mengze.vip/gh/YShenZe/Blog-Static-Resource@main/images/IMG20250320090451.jpg)
![IMG20250320090438.jpg](https://cdn.mengze.vip/gh/YShenZe/Blog-Static-Resource@main/images/IMG20250320090438.jpg)
![IMG20250320090404.jpg](https://cdn.mengze.vip/gh/YShenZe/Blog-Static-Resource@main/images/IMG20250320090404.jpg)

好的你已经完成了pkg软件源的切换和软件包列表更新，那么接下来就使用这条指令安装必要的软件包和工具：
```sh
pkg install git cmake make python clang libandroid-execinfo git-lfs
```
![IMG20250320091121.jpg](https://cdn.mengze.vip/gh/YShenZe/Blog-Static-Resource@main/images/IMG20250320091121.jpg)
输入Y并回车继续这其实就是再问你是否需要确定安装。

## 克隆并编译llama.cpp
使用如下指令克隆llama仓库并进入文件夹：
```sh
git clone https://github.com/ggerganov/llama.cpp #克隆仓库
cd llama.cpp #进入文件夹
```
使用Cmake对llama.cpp进行编译
```sh
make -j4
```
这里的 -j4 表示使用4个线程进行编译，可根据设备的CPU核心数调整，理论上线程越多编译越快。

编译过程中我遇到了新的问题，我找到了解决方案，如果你也遇到这个问题 看这里解决
这是因为llama.cpp项目已经弃用了旧的Makefile构建方式，现在推荐使用CMake构建。
先新建一个build目录，然后进入该目录
```sh
mkdir build
cd build
```
再使用如下命令来配置构建
```sh
cmake ..
```
然后再使用这个命令进行编译
```sh
make -j4
```
编译完成之后即可开始安装
```sh
make install
```

## 下载并运行模型
使用如下指令安装git-lfs
```sh
git lfs install
```

然后运行这个指令下载`Qwen-1.5-0.5B`开源大模型：
```sh
git clone https://gitcode.com/hf_mirrors/Qwen/Qwen1.5-0.5B-Chat-GGUF.git
```

预计会下载4GB左右，下载过程比较慢，需耐心等待，确保网络保持通畅

完成下载之后也基本上完成了，接下来就可以运行`Qwen-1.5-0.5B`开源大模型了，参考如下指令：

```sh
./main -m qwen1_5-0_5b-chat-q8_0.gguf -n 512 --color -i -cml -f prompts/chat-with-qwen.txt
```

然后就是我使用的工具是llama.cpp，如果你用不了也可以使用huggingface-cli，使用如下指令：
```sh
pip install huggingface_hub
```

安装好之后可以使用这一串指令下载并运行模型
```sh
huggingface-cli download Qwen/Qwen1.5-0.5B-Chat-GGUF qwen1_5-0_5b-chat-q8_0.gguf --local-dir . --local-dir-use-symlinks False
```